---
title: "MACS 30500 LECTURE 16"
author: "Topics: Getting data from the web: API."
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "26px",
  code_font_size = "22px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
```

```{r pkgs, include = FALSE, cache = FALSE}
library(tidyverse)
library(broom)
library(jsonlite)
library(stringr)
library(XML)
library(curl)
library(repurrrsive)
library(listviewer)
library(wordcloud)
library(tidytext)
library(manifestoR)
library(httr)
library(rtweet)
library(viridis)
library(flipbookr)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

class: inverse, middle

## Agenda

+ Review of last lecture

+ Scraping using an API
  + Terminology and sending queries to APIs
  + APIs with and without a wrapper package
  + Registering for access

+ Rectangling or Simplifying lists

+ API or direct scraping?

+ Scraping challenges, tips, and ethics

---

class: inverse, middle

## Review

---

## Last time we talked about two main topics...

<br>

### 1. How computers interact on the web

1. Your browser (e.g., Chrome, Safari, etc.) sends a request to the server hosting the website.
1. The website's server responds to your browser by sending content written in HTML.
1. Your browser translates the HTML content into the website you see.

--

**Web scraping happens in step 2:** 

* you write code in R that grabs the server's HTML response: this is done with the function `read_html()` in `rvest`
* then you write more code to extract content from it: there are several functions that do that in `rvest` like `html_elements()`, `html_text2_()`, etc.

---

## Last time we talked about two main topics...

<br>

### 2. What is behind a website

* A website is mainly made of HTML, CSS, and JS code
* The most important is the HTML: the website's "skeleton", made of tags (and their attributes) that follow a hierarchical tree-like structure 
* CSS code builds on top of HTML: it makes the website pretty by using tags as "anchors" to style the website

---

## We also learned there are two main ways to scrape

<br>

### 1. Directly scraping the website

* Every website is built with code, typically a mix of HTML, CSS, and JavaScript.
* To collect data from a website, we need to learn how to interact with this code.
* Example: in theory, anything! in practice, start simple like Wikipedia or government sites.


--

### 2. Using a web API (Application Programming Interface)

* Interface provided by the website that allows users to collect data from it.
* To collect data from a website using an API, you need to learn how to use that API.
* Example: [OMDb API](https://omdbapi.com/), see chapter 4 from today's readings.

---


## Then we practiced "direct" scraping in R with `rvest`

**`rvest`documentation:** https://rvest.tidyverse.org/ 

**Web scraping 101:** https://rvest.tidyverse.org/articles/rvest.html#reading-html-with-rvest

**SelectorGadget:** https://rvest.tidyverse.org/articles/selectorgadget.html (see also the previous lecture class materials on this!)

**List of Functions in `rvest`:** https://rvest.tidyverse.org/reference/index.html
    
#### Example: scraping Presidential statements

  > Review the materials from the previous lecture! 


---

class: inverse, middle

## Scraping using an API: Application Programming Interface

Today we talk about the second way of scraping: using an API!

---

### API: terminology

  > Interface provided by the website that allows users to collect data from that website. 

The majority of web APIs use a particular style know as **REST** or **RESTful** which stays for "Representational State Transfer." 

**REST** allows to query the website database using URLs, just like you would construct an URL to view a web page.

An **URL** (Uniform Resource Location), is a string of characters that uses HTTP (HyperText Transfer Protocol) to send request for data. 

---

### Sending queries to an API  

The process described in the `macss` example (last lecture, [here](https://computing-soc-sci.netlify.app/slides/getting-data-from-the-web-scraping/#12)) is very similar to how APIs work, with only a few changes:

* The URL that you use for sending requests (queries) to an API will have two more things: search terms and/or filtering parameters, and specific references to that API

* The response you get back from the API is not formatted as HTML, but it will likely be a raw text response

* You need R to interact with the API. For example:
 * parse that response, get the data from it
 * convert them into a format that you like (dataframe, lists, etc.)
 * then export the data, usually as `.csv` or `.json`

---

### API: with and without a wrapper package

When using an API in R, there are two approaches:

**1. Using an API wrapper:** using an API through an R package that someone has specifically designed to interact with that API. The package acts as a wrapper for the API. You use R to interact with the wrapper.

**2. Direct API interaction**: if no R wrapper exists, you can directly use the API provided by the website. In this case, you use R to communicate directly with the website's API.

--

Sometimes both approaches are available. Other times, only approach 2 is available. 

**Tip:** Scraping with an API, compared to direct scraping, can be a hit-or-miss experience. The keys to success are using a well-designed R wrapper if available or relying on well-documented API documentation.

*Let's examine both approaches in more detail!*

---

### Using an API with a wrapper package

* A wrapper is a specific package written in a given language, like R, for an existing API (tailored to that API only)

* Useful because: reproducible, up-to-date (ideally), easy to access
    
* How to use: each package should come with documentation (in the form of a pdf, GitHub repo, or both) that explains how to use it and its main functions. Reading the documentation is essential to understand how to effectively use the package.

---

### Using an API with and without a wrapper package: Examples

The class materials for today (download it from the website) include:

* **Four examples of APIs that have a wrapper package:**
  * Wordbank
  * GeoNames
  * Census
  * Manifesto Project

* **One example of an API without a wrapper:** 
  * OMDb Open Movie Database

<!--
Not all APIs have an R wrapper package to facilitate interaction. If such a wrapper does not exist, we can interact directly with the API, though this can be though for beginner projects (depending on the API).

Direct interaction typically requires more coding, such as writing custom functions, but it also offers greater flexibility to customize data requests.
-->

---

### Using an API with a wrapper: Wordbank API example

**Wordbank database API**: `wbstats` R wrapper package
* socioeconomic indicators spanning several decades and numerous topics
* full data are available for bulk download as CSV files from their website (see HW5); however, researcher often only need a handful of indicators or a subset of countries
* thus the Wordbank database has an API and `wbstats` is an R wrapper that simplify using the API; it returns the results in a tidy data frame! 

*Let's move to today's in-class materials to learn more about this API!*

---

### Register for access: Overview

The Wordbank API is free and does not require registration. However **many APIs require you to register for access:** 

* Sometimes, registration is as simple as providing an email and password, then receiving an email with your username and private API key.
* Other times, you need to submit an application and go through a review process.
* Often, this process is free, but some APIs require paying a fee.

**Why register for access?** Registration allows APIs to track users, their queries, and manage demand. 

If an API requires you to register and obtain a username, password, or key, you will need to provide this same information when using the corresponding R wrapper package.

---

### Register for access: GeoNames example

**GeoNames geographical database API**: 

* geographical information for all countries and other locations
* the API requires you to register and set a username and key
* the `geonames` package provides a wrapper for R

*Let's check today's in-class materials to learn how store username and other private info in R for this API!*

<!-- we are going to register to this API and show how to store our credientials in R -->

---

### Using an API with a wrapper: More examples

There are two more examples of using an API with a wrapper included in your class materials: 

**Census Bureau API**: `tidycensus` R wrapper package
* statistical data from the US Census Bureau
* the `tidycensus` provides a R wrapper for the US Census Bureau’s Census and five-year American Community Survey APIs

**Manifesto Project API**: `manifestoR` R wrapper package
* political party manifestos from around the world
* covers 1,000+ parties from 1945 until today in 50+ countries on five continents. 
* the `manifestoR` package provides a wrapper for R

They both require you to register a free account!

---

### Using an API without a wrapper: OMDb Movies example

**OMDb** stores info about movies, and currently does provide a very good API but not an R wrapper

Website: https://www.omdbapi.com/

*See the `api_omdb.Rmd` tutorial in today's in-class materials for more!*

---

class: inverse, middle

## Rectangling or Simplifying lists

---

### Rectangling

In R "Rectangling" means **transforming non-rectangular data (often nested lists) into a rectangular format (often a data frame).** 

The term is often associated with the `tidyverse` and its principles of tidy data.

--

**In the context of web scraping,** "rectangling" means transforming a deeply nested list (often obtained from raw JSON or XML data) into a tidy data set with rows and columns that is easier to work with!

If you need to simplify nested lists for your scraping project *check today's class materials for a tutorial and examples of rectangling!*

---

class: inverse, middle

## API or direct scraping? 

---

### Using an API VS. direct scraping

API pros:
* You comply with website preferences (if a website has an API, it wants you to use it)
* Sometimes using the API is the only option you have (the website may make direct scraping difficult or impossible)
* if the API has a good R wrapper, you can get data more easily than by scraping directly

API cons:
* API usually requires you to register
* rate-limit
* time invested to learn how to use the API (each website has its own API)

---

### Using an API VS. direct scraping

Scraping pros:
* can be powerful
* basic rules can be applied to any scraping project

Scraping cons:
* inconsistent and messy
* susceptible to site changes (e.g. your code for scraping will break)

---

class: inverse, middle

## Scraping tips, challanges, and ethics 

---

### Scraping: tips

* Confirm that there is no R package and no API
* Make sure your code scrapes only what you want (and no additional information)
* Slow down your scraper with `Sys.sleep()`
* Save and store the content of what you scrape, so to avoid scraping it again

---

### Scraping: Challenges & Solutions

* **Variety:** every website is different, so every website requires a new project

* **Bad formatting**: websites should be built using "logical formatting" following a properly nested HTML structure. In practice, that's often not the case

* **Change:** the same website might change over time, so you might find that your code of a few months ago does not work anymore. The good news is that, usually, it takes only a few changes to run it again

* **Limits:** some websites set a max amount of data you can scrape at once, for example 50 pages or 2500 articles max. The solution is to break your requests into "chunks"

* **Messy:** the scraped data are usually a bit messy, and they need to be cleaned (regex is helpful here)

* **Dynamic Scraping:** this not really a challenge but something to keep in mind: many websites incorporate Javascript dynamic parts, which require advanced scraping skills

---

### Scraping: Ethics

* **Private data (not OK) VS. Public data (OK):** If there is a password, it is private data. For example, it is not OK to scrape data from an online community where only logged-in users can access posts (unless you use the API provided by the website and follow its rules). **In general: if the website has an API, use it.**

* **Check the `robots.txt`** before you scrape by adding `/robots.txt` at the end of your URL. For example, for the NYT Robot File, type: `https://www.nytimes.com/robots.txt`. The star after "User-agent" means "the following is valid for all robots"; things you cannot scrape are under "Disallow". More [here](https://www.robotstxt.org/robotstxt.html)

* **Read the website’s Terms of Service (ToS):** legal rules you agree to observe in order to use a service. Violating ToS exposes you to the risk of violating CFAA or "Computer Fraud & Abuse Act", which is a federal crime.

* **If you are scraping lots of data:** use `rvest` together with `polite`. The polite package ensures that you’re respecting the `robots.txt` and not hammering the site with too many requests.

<!--
### Acknowledgments 

APIs examples are drawn from Rochelle Terman’s "Finding APIs" page [here](https://plsc-31101.github.io/course/collecting-data-from-the-web.html#finding-apis) and Benjamin Soltoff’s “Computing for the Social Sciences” course materials, licensed under the CC BY NC 4.0 Creative Commons License. Any errors or oversights are mine alone.
-->