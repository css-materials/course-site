---
title: "Getting data from the web: scraping"
author: "MACSS 30500 <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "26px",
  code_font_size = "22px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
```

```{r pkgs, include = FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE
)

library(tidyverse)
library(rvest)
library(lubridate)

set.seed(1234)
theme_set(theme_minimal(base_size = rcis::base_size))
```

class: inverse, middle

# Agenda

* Web Scraping Intro: 
  * Definition
  * Two ways to scrape data from the web: directly or via API

* Two Key Concepts:
  * How the web works
  * What is behind a website

+ Direct scraping in R with `rvest`

<!-- TO IMPROVE FOR NEXT TIME

Improvement 1:
Good idea to start by the end, that is running the president scraper
and see what we get back, but this time prepare a simple R script 
the script should be without explanation and should gather the 
title, date, name, and message without a function
just to demo the code
do not share it with students just run it in class

Improvement 2:
do a short intro to rvest
show the documentation
the demo https://rvest.tidyverse.org/articles/rvest.html#reading-html-with-rvest
and list the main functions
+ spend some time on how to extract tags (for example use the title as example 
to show how you can use different code to extract the same tag)

for me before class: check the meaning of the dot 
and the structure that rvest expects
to pass tags, check also html_node() function

Improvement 3:
Take the current demo and modify it so to include the improvements above,
it should resemble what you teach for the Python course
could do this as a mini module in week 9 of the course 
decide how in depth you want to go with the challenges and develop code for them
ADD 3 LECTURES ON SCRAPING!

Improvement 4:
add code on how to retrieve info on a mac -  DONE

-->

---

class: inverse, middle

## Web Scraping Intro

---

## What is web scraping?


**Web scraping is the process of collecting information from a website.** If you've ever copied and pasted info from the Internet, you've done the same task as a scraper, just manually and on a small scale. Web scraping automates and scales up this process!

**Examples:**

* companies' names, emails, and phone numbers 
* newspaper articles 
* products' prices, descriptions, and characteristics 
* real estate data
* social media (comments, likes, followers, customer reviews, etc.)
* selling or exchange of goods websites (craigslist, etc.)
* forums
* etc.

---

```{r fig.align = "center", echo = FALSE, out.width = "70%"}
knitr::include_graphics(path = "webscraping.png", error = FALSE)
```

Image Source at [this link](https://medium.com/analytics-vidhya/web-scraping-in-python-for-data-analysis-6bf355e4fdc8)

---
<!-- Add one slide that says why we study webscraping in this class -->

## Two ways to scrape data from the web

<br>

### 1. Directly scraping the website

* Every website is built with code, typically a mix of HTML, CSS, and JavaScript.
* To collect data from a website, we need to learn how to interact with this code.
* Example: in theory, anything! in practice, start simple like Wikipedia or government sites.


--

### 2. Using a web API (Application Programming Interface)

* Interface provided by the website that allows users to collect data from it.
* To collect data from a website using an API, you need to learn how to use that API.
* Example: [OMDb API](https://omdbapi.com/), see chapter 4 from today's readings.

---

## Our plan

**Today** we focus on directly scraping websites: 
* we cover some theory and key concepts
* we practice!

**Next lecture** we focus on scraping using APIs.


---

class: inverse, middle

## Two Key Concepts...

<!--
1.  **How the web works** (e.g., how computers interact with each other on the web)

1.  **What is behind a website** (e.g., the code websites are made of)
-->

---

class: inverse, middle

## Concept 1: How the web works

---

### How do computer interact on the web? Theory

Computers communicate on the web by sending and receiving **data requests** (GET) and **data responses** (POST). Every computer has an address that other computers can refer to.

When you click on a webpage, your **web browser** (e.g., Chrome, Safari) sends a data request to the **web server** of that page (a database where all the info about that page are stored) and receives a response.

```{r fig.align = "center", echo = FALSE, out.width = "50%"}
knitr::include_graphics(path = "request_response.png", error = FALSE)
```

Image Source [at this link](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor)

---

### How do computer interact on the web? Example

Navigating the web basically means **sending requests to different servers and receiving back various files**, mainly written in HTML and other languages.

For example, if you type `https://macss.uchicago.edu/current-student-resources` into your web browser and hit enter, these steps occur behind the scenes:

1. your web browser (Chrome, Safari, etc.) translates what you typed into a HTTP request. That request tells the web server that stores all `macss` info that you want to access the specific piece of info stored at `/current-student-resources`

1. the web server that hosts `macss` receives your request and sends back to your web browser an response and the response content (a bunch of files written in HTML)

1. your browser transforms the response content into a nice visual display that might include texts, graphics, hyperlinks, etc.

--

**When we scrape data, we use R packages that perform these steps for us (via APIs or without)!**

---

class: inverse, middle

## Concept 2: What is behind a website

---

### What is behind a website

A website is made of the following elements:

+ **HTML**, which means *HyperText Markup Language*, is the core element of a website. HTML uses tags to organize the webpage (i.e., makes the text bold, creates paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden

+ **CSS**, which means *Cascading Style Sheets*: to make the page looks nice

+ **Javascript (JS)** code: to add interactive elements to the page (you need "dynamic web scraping" techniques to scrape JS code, not covered in this course)

+ **Other stuff** such as images, hyperlinks, videos or multimedia

---

### What is behind a website

Knowing how read the HTML (and CSS) language, is fundamental when we crape the website directly. When we use an API and/or an API wrapper, this is less important, but still useful. 

Our goal: lean how to read HTML and CSS languages

---

### HTML: HyperText Markup Language

* most important thing for web scraping: makes the "skeleton" or structure of a website
* it is made of tags 
* looks messy, but follows a hierarchical [tree-like structure](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108) of tags within tags 

Standard HTML syntax:
```
   <html>
     <head>
        <title>general info about the page</title>
     </head>
     <body>
       <p>a paragraph with some text about the page</p>
       <p>another paragraph with more text</p>
       <p>ciao</p>
     </body>
   </html>
   
```

---

### HTML Tags

* tags are organized in a **tree-like structure** and are nested within each other

* tags **go in pairs** one on each end of the content that they display: 
   * example: `<p>ciao</p>` only the word "ciao" shows up on the webpage, and the `/` signals the end of the tag

* tags can have **attributes** which provide more info: 
    * example: `<p>ciao id='first'</p>`
    * there are two important attributes for scraping: `id` and `class`; these two HTML attributes are used together with CSS elements to control the visual appearance of the page
    
--

Let's examine the HMTL tags structure more closely!

<!-- add more info here
https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping
* more frequently used tags
* more on tags attributes
* CSS 
* CSS and HTML
* see staff from my Python course, lecture 2 (e.g. every page is different, no perfect structure,etc.)
-->

---

### HTML Tags

* `html`
    * `head`
        * `title`
        * `a href`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

### HTML Tags

* `html`
    * `head`
        * `title`
        * `a href` -- links
        * `script` -- code
    * `body`
        * `div` -- generic container for content (block)
            * `p` -- paragraph
                * `b` -- bold formatting
            * `span` -- generic container for content (in line)
        * `table`
            * `tr` -- row of cells
                * `td` -- actual cell element 
                * `td`
        * `img`


---

### HTML Tags: example

.small[
```html
<html>
  <head>
    <title>Title</title>
    <a href="http://github.com">GitHub</a>
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```
]
Find: (1) the text content `Frozen`; (2) the GitHub link and the text content `GitHub`

---

### HTML Tags: example

**Find the text content `Frozen`:**

--

```html
<span>Frozen</span>
```

* `<span></span>` - tag name
* `Frozen` - content as text

<!--In this ex. there is only one span tag: what if the HTML has multiple span tags?-->

--

**Find the GitHub link and text content `GitHub`:**

--

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - tag attribute (argument)
* `"http://github.com"` - tag attribute (value)
* `GitHub` - content as text

---

### CSS: Cascading Style Sheet

Often a website has HTML tags (with attributes) and CSS elements:
* **HTML defines the content** 
* **CSS defines the format** 

This is an example of CSS code. Notice: (1) it is different from HTML code; and (2) the `span` HTML tag can be styled using CSS `color`: 

```css
span {
  color: #ffffff; }
```
--

**Most websites use HTML tags with `class` and `id` attributes to provide “hooks” for CSS elements**. This way the CSS "knows" where to apply CSS stylistic elements.

Example: https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/class

<!-- https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping 
[CSS diner](http://flukeout.github.io) 

### HTML + CSS: example 

.pull-left[

```html
<body>
    <table id="content">
        <tr class='name'>
            <td class='firstname'>
                Kurtis
            </td>
            <td class='lastname'>
                McCoy
            </td>
        </tr>
        <tr class='name'>
            <td class='firstname'>
                Leah
            </td>
            <td class='lastname'>
                Guerrero
            </td>
        </tr>
    </table>
</body>
```

]

.pull-right[

Find the elements to use to extract:
1. The entire table
1. The general element(s) containing first names
1. Just the specific element "Kurtis"

]

-->

---

### HTML + CSS: further resources

* Great intro guide: `rvest` page "Web scraping 101" https://rvest.tidyverse.org/articles/rvest.html

* Using HTML and CSS for scraping: https://sscc.wisc.edu/sscc/pubs/webscraping-r/html.html

* Handy references for your scraping projects:

  * HTML overview: https://www.w3schools.com/html/html_intro.asp
  * List of tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element 

* See our website (today's lecture) for more!

---

class: inverse, middle

## Practice all of this in R with the `rvest` package

* `rvest`: https://rvest.tidyverse.org/ 
* Example: scraping presidential statements

---

### Using `rvest` to read HTML

**The R package [`rvest`](`https://rvest.tidyverse.org/`) allows us to:**
1. Collect and read the HTML source code of a webpage
1. Find and keep the specific HTLM/CSS elements that we want from that webpage using HTML tags or attributes, and using CSS selectors

--

**Our Example: Presidential statements**

We are going to scrape data from this URL: `https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration`

Download today's class materials to follow along!

