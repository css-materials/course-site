<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MACS 30500 LECTURE 14</title>
    <meta charset="utf-8" />
    <meta name="author" content="Topics: Getting data from the web by scraping." />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# MACS 30500 LECTURE 14
]
.author[
### Topics: Getting data from the web by scraping.
]

---






class: inverse, middle

# Agenda

* Web Scraping Intro: 
  * Definition
  * Two ways to scrape data from the web: directly or via API
  * Our goal today: scrape U.S. Presidential Statements

* Two Key Concepts:
  * How the Web Works
  * What is Behind a Website

+ The `rvest` package

&lt;!-- TO IMPROVE FOR NEXT TIME NOTES FALL 2024

Keep starting from final product as R script

Cover the sys.sleep in this lecture especially if they plan to scrape lots of things

Mention more clearly what you can do with the data you scraped from the US Presidential especially the text. In this case, count since I plan to scrape 4000+ letters. Remember to adjust the in-class tutorials with the challenge and change lang-age. Assign challenge as homework? 

Cover more clearly how to navigate html tag structure
* add one example of code in the slides (like use rvest syntaxt to scrape github text and link from there)
* take example from tutorial 2 on scraping on tags and introduce it here
* say that scraping can be messy bcs websites are not always nicely buit

Go over the other in-class example with the housing website data so students can have more ideas on what to scrape

Introduce in the slides html_table() as some students might need to use it

Maybe replace the API scraping with text mining and get rid of it altogether? Or briefly introduce it in lecture 2 on scraping? 
--&gt;

---

class: inverse, middle

## Web Scraping Intro

---

## What is web scraping?


**Web scraping is the process of collecting information from a website.** If you've ever copied and pasted info from the Internet, you've done the same task as a scraper, just manually and on a small scale. Web scraping automates and scales up this process!

**Examples:**

* companies' names, emails, and phone numbers 
* newspaper articles 
* products' prices, descriptions, and characteristics 
* real estate data
* social media (comments, likes, followers, customer reviews, etc.)
* selling or exchange of goods websites (craigslist, etc.)
* forums
* etc.

---

&lt;img src="webscraping.png" width="70%" style="display: block; margin: auto;" /&gt;

Image Source at [this link](https://medium.com/analytics-vidhya/web-scraping-in-python-for-data-analysis-6bf355e4fdc8)

---
&lt;!-- Add one slide that says why we study webscraping in this class --&gt;

## Two ways to scrape data from the web

&lt;br&gt;

### 1. Directly scraping the website

* Every website is built with code, typically a mix of HTML, CSS, and JavaScript.
* To collect data from a website, we need to learn how to interact with this code.
* Example: in theory, anything! in practice, start simple like Wikipedia or government sites.

---

## Two ways to scrape data from the web

&lt;br&gt;

### 2. Using a web API (Application Programming Interface)

* Interface provided by the website that allows users to collect data from it.
* To collect data from a website using an API, you need to learn how to use that API.
* Example: [OMDb API](https://omdbapi.com/), see chapter 4 from today's readings.

&lt;!--
## Our plan

**Today** we focus on directly scraping websites: 
* we cover some theory and key concepts
* we practice!

**Next lecture** we focus on scraping using APIs.
--&gt;

---

### Our goal today: Scraping U.S. Presidential Statements 

Before diving into web scraping concepts, let's first observe today's end goal:

**U.S. Presidential statements &lt;https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration&gt;**

From this page, we will scrape the following info:
* speaker name
* date
* title
* text

&lt;!-- demo this in class, no sharing code; copy and paste your code into workbench --&gt;

---

class: inverse, middle

## Two Key Concepts...

**Concept 1: How the Web Works** (e.g., how computers interact with each other on the web)

**Concept 2: What is Behind a Website** (e.g., understand what makes up a website)

*These theoretical concepts apply to any scraping project, not just R!*

---

class: inverse, middle

## Concept 1: How the Web Works

---

### How do computer interact on the web? Theory

Computers communicate on the web by sending and receiving **data requests** (GET) and **data responses** (POST). Every computer has an address that other computers can refer to.

When you click on a webpage, your **web browser** (e.g., Chrome, Safari) sends a data request to the **web server** of that page (a database where all the info about that page are stored) and receives a response.

&lt;img src="request_response.png" width="50%" style="display: block; margin: auto;" /&gt;

Image Source [at this link](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor)

---

### How do computer interact on the web? Example

Navigating the web basically means **sending requests to different servers and receiving back various files**, mainly written in HTML and other languages.

For example, if you type `https://macss.uchicago.edu/current-student-resources` into your web browser and hit enter, these steps occur behind the scenes:

1. your web browser (Chrome, Safari, etc.) translates what you typed into a HTTP request. That request tells the web server that stores all `macss` info that you want to access the specific piece of info stored at `/current-student-resources`

1. the web server that hosts `macss` receives your request and sends back to your web browser a response code (200 for yes, etc.) and the response content (files written in HTML)

1. your browser transforms the response content into a nice visual display that might include texts, graphics, hyperlinks, etc.

--

**When we scrape data, we use R packages that perform these steps for us (via APIs or without)!**

---

class: inverse, middle

## Concept 2: What is Behind a Website

---

### What is Behind a Website

A website is made of the following elements:

+ **HTML** *HyperText Markup Language*, is the core element of a website. HTML uses tags to organize the webpage (i.e., creates paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden

+ **CSS** *Cascading Style Sheets*: improves the look of the page

+ **Javascript** *JS* code adds interactive elements to the page (you need "dynamic web scraping" techniques to scrape JS code, not covered in this course)

+ **Other stuff** such as images, hyperlinks, videos or multimedia

---

### What is Behind a Website

When we scrape a website directly: knowing how read the HTML (and CSS) language, is fundamental  

When we use an API and/or an API wrapper: less important, but still useful. 

Our goal: lean how to read HTML and CSS languages

---

### HTML: HyperText Markup Language

* most important thing for web scraping: makes the "skeleton" or structure of a website
* it is made of tags 
* looks messy, but follows a hierarchical [tree-like structure](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108) of tags within tags 

Standard HTML syntax:
```
   &lt;html&gt;
     &lt;head&gt;
        &lt;title&gt;general info about the page&lt;/title&gt;
     &lt;/head&gt;
     &lt;body&gt;
       &lt;p&gt;a paragraph with some text about the page&lt;/p&gt;
       &lt;p&gt;another paragraph with more text&lt;/p&gt;
       &lt;p&gt;ciao&lt;/p&gt;
     &lt;/body&gt;
   &lt;/html&gt;
   
```

---

### HTML Tags

HTML tags are organized in a **tree-like structure** and are nested within each other

HTML tags **go in pairs** one on each end of the content that they display. For example: `&lt;p&gt;ciao&lt;/p&gt;` only the word "ciao" shows up on the webpage, the `/` signals the end of the tag

HTML tags can have **attributes** which provide additional info about the tag. For example: `&lt;p&gt;ciao id='first'&lt;/p&gt;`
   * `&lt;p&gt;` is the tag
   * `&lt;id&gt;` is the attribute
   * two important attributes for scraping: `id` and `class`; they are used by CSS elements to control the visual appearance of the page
    
--

Let's examine the HMTL tags structure more closely!

&lt;!-- add more info here
https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping
* more frequently used tags
* more on tags attributes
* CSS 
* CSS and HTML
* see staff from my Python course, lecture 2 (e.g. every page is different, no perfect structure,etc.)
--&gt;

---

### HTML Tags

* `html`
    * `head`
        * `title`
        * `a href`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

### HTML Tags

* `html`
    * `head`
        * `title` - title
        * `a href` - links
        * `script` - code
    * `body`
        * `div` - generic container for content (block)
            * `p` - paragraph
                * `b` - bold formatting
            * `span` - generic container for content (in line)
        * `table`
            * `tr` - row of cells
                * `td` - actual cell element 
        * `img` - image


---

### HTML Tags: example

.small[
```html
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Title&lt;/title&gt;
    &lt;a href="http://github.com"&gt;GitHub&lt;/a&gt;
    &lt;script src="https://c.js"&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;
      &lt;p&gt;Click &lt;b&gt;here&lt;/b&gt; now.&lt;/p&gt;
      &lt;span&gt;Frozen&lt;/span&gt;
    &lt;/div&gt;
    &lt;table style="width:100%"&gt;
      &lt;tr&gt;
        &lt;td&gt;Kristen&lt;/td&gt;
        &lt;td&gt;Bell&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/table&gt;
  &lt;img src="http://ia.media-imdb.com/images.png"/&gt;
  &lt;/body&gt;
&lt;/html&gt;
```
]
Find: (1) the text content `Frozen`; (2) the GitHub link and the text content `GitHub`

---

### HTML Tags: example

**Find the text content `Frozen`:**

--

```html
&lt;span&gt;Frozen&lt;/span&gt;
```

* `&lt;span&gt;&lt;/span&gt;` - tag name
* `Frozen` - content embedded in the tag as text

&lt;!--In this ex. there is only one span tag: what if the HTML has multiple span tags?--&gt;

--

**Find the GitHub link and text `GitHub` embedded within it:**

--

```html
&lt;a href="http://github.com"&gt;GitHub&lt;/a&gt;
```

* `&lt;a&gt;&lt;/a&gt;` - tag name
* `href` - tag attribute (argument)
* `"http://github.com"` - tag attribute (value)
* `GitHub` - content as text

---

### CSS: Cascading Style Sheet

Often a website has HTML tags (with attributes) and CSS elements:
* **HTML defines the content** 
* **CSS defines the format** 

This is an example of CSS code. Notice: (1) it is different from HTML code; and (2) the `span` HTML tag can be styled using CSS `color`: 

```css
span {
  color: #ffffff; }
```
--

**Most websites use HTML tags with `class` and `id` attributes to provide “hooks” for CSS**. This way the CSS "knows" where to apply CSS stylistic elements.

Example: https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/class

&lt;!-- https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping 
[CSS diner](http://flukeout.github.io) 

### HTML + CSS: example 

.pull-left[

```html
&lt;body&gt;
    &lt;table id="content"&gt;
        &lt;tr class='name'&gt;
            &lt;td class='firstname'&gt;
                Kurtis
            &lt;/td&gt;
            &lt;td class='lastname'&gt;
                McCoy
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr class='name'&gt;
            &lt;td class='firstname'&gt;
                Leah
            &lt;/td&gt;
            &lt;td class='lastname'&gt;
                Guerrero
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;
&lt;/body&gt;
```

]

.pull-right[

Find the elements to use to extract:
1. The entire table
1. The general element(s) containing first names
1. Just the specific element "Kurtis"

]

--&gt;

---

class: inverse, middle

## The `rvest` package in R

---

### Using `rvest`

**The R package [`rvest`](`https://rvest.tidyverse.org/`) is a package designed for scraping project and allows us to:**

1. Read and save in an object the HTML source code of a webpage

1. Find the specific HTLM and CSS elements from a webpage (using HTML tags or attributes, and using CSS selectors)

---

### Main `rvest` functions

+ **`read_html()`**
+ **`html_elements()`**
+ **`html_text2()`**
+ **`html_attr()`** 
+ **`html_table()`** 

Source: https://rvest.tidyverse.org/index.html#usage

---

### Main `rvest` functions

**`read_html()`** reads an html page into R:
```
library(rvest)
starwars &lt;- read_html("https://rvest.tidyverse.org/articles/starwars.html")
```

**`html_elements()`** finds elements that match a CSS selector or XPath expression. Here each section corresponds to a different film:
```
films &lt;- starwars %&gt;% html_elements("section")
films
```

---

### Main `rvest` functions

**`html_text2()`** extracts text embedded within tags. Here the title is given by the text inside `&lt;h2&gt;`:
```
title &lt;- films %&gt;% 
  html_element("h2") %&gt;% 
  html_text2()
title
```

**`html_attr()`** gets data out of attributes; always returns a string so we convert it to an integer using a `readr::parse_integer()`
```
episode &lt;- films %&gt;% 
  html_element("h2") %&gt;% 
  html_attr("data-id")
episode
```

---

class: inverse, middle

## Practice all of this in R with `rvest`

**We scrape this U.S. Presidential statements page: &lt;https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration&gt;**

Through this example, we learn the following:
* use `rvest` functions to collect name, date, title, and text from that page
* test two methods to identify tags: examining the webpage directly, and using tools that assist us (e.g., SelectorGadget)
* write a function to scale up our scraper to handle multiple pages

Download today's class materials to follow along!

---

### Web Scraping Resources

* `rvest` documentation and tutorials:
  * Web scraping 101: &lt;https://rvest.tidyverse.org/articles/rvest.html&gt;
  * Tutorial: &lt;https://rvest.tidyverse.org/articles/rvest.html#html-basics&gt;

* Using HTML and CSS for scraping: https://sscc.wisc.edu/sscc/pubs/webscraping-r/html.html

* Handy references for your scraping projects:

  * HTML overview: https://www.w3schools.com/html/html_intro.asp
  * List of tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element 
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLines": true,
"highlightLanguage": "r",
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
