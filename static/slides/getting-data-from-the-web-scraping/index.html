<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Getting data from the web: scraping</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACSS 30500   University of Chicago" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Getting data from the web: scraping
]
.author[
### MACSS 30500 <br /> University of Chicago
]

---






class: inverse, middle

# Agenda

+ Web scraping: definition
+ Two ways to scrape data:
  1. using an API
  1. directly
+ Key concepts:
  + what is behind a website
  + how the web works
+ Direct scraping with `rvest`
+ Scraping challenges, tips, and ethics 

---


class: inverse, middle

## Getting data from the web

---

&lt;img src="webscraping.png" width="70%" style="display: block; margin: auto;" /&gt;

Image Source at [this link](https://medium.com/analytics-vidhya/web-scraping-in-python-for-data-analysis-6bf355e4fdc8)

---

## Web scraping: definition

Web scraping is **the process of collecting or "scraping" information from a website**. 

If you have ever copied and pasted information from the Internet, you have performed the same task as any scraper, just on a small scale! Web scraping allows to automate and scale up this process.

--

Examples: 
* companies' names, emails, and phone numbers 
* newspaper articles 
* customer reviews
* products' prices, descriptions, and characteristics 
* real estate data
* statistical data
* social media data (comments, likes, followers, etc.)
* selling or exchange of goods websites (craigslist)
* forums
* etc.

---

class: inverse, middle

## Two ways to scrape data from the web
  ### 1: Using a web API
  ### 2: Directly scraping the website

---

### Two ways to scrape data from the web. Option 1: Using a web API

  &gt; A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.

--

Example: [OMDb API](https://omdbapi.com/) see assigned reading chapter 4 

You can use an API with or without an R wrapper: 
* a wrapper is a specific R package written by someone to interact with a specific API 
* tip for beginners: scrape with an API only if an R wrapper is available AND is well done

---

### Two ways to scrape data from the web. Option 2: Direct scraping

  &gt; Every website is made of code. This code is a mix of HTML, CSS and Javascript code. Therefore, each website, and the information stored in it, is directly accessible by users (us!). We only need to know how to interact with that code. 

--

Given that every website is made of code, scraping a website directly (aka without using an API) is technically always possible. However:
1. if a website has an API, the general rule is to use it
2. check the website's rules for scraping (ToS and `robots.txt`)

Example: anything on the web! Scraping something from Wikipedia is a common beginners project

---

### Advantages and Disadvantages of each approach

Scraping a website directly has a number of benefits and challenges compared to using an API.

API pros:
* you comply with website preferences
* sometimes using the API is the only option you have (the website can make it very difficult to scrape it without an API)
* if the API has a good R wrapper (a R package that allows us to interact with the API), it might be easier than scraping directly

API cons:
* API usually requires you to register
* rate-limit
* time invested to learn how to use the API

---

### Advantages and Disadvantages of each approach

Scraping pros:
* more powerful

Scraping cons:
* inconsistent and messy
* susceptible to site changes (e.g. your code for scraping will break)

---

## Our plan

Today we focus on direct scraping. Next lecture we focus on scraping using APIs.

But before we do any of these, we need to familiarize with two concepts:

&gt; Concept 1: **How the web works** (e.g., how computers interact with each other on the web)

&gt; Concept 2: **What is behind a website** (e.g., the code websites are made of)

---

class: inverse, middle

## Concept 1: How the web works (e.g., how computers interact on the web)

---

### How do computer interact on the web? Theory

Computers talk to each other on the web by sending and receiving (GET) **data requests** and (POST) **data responses**: some making requests, some receiving and answering them, some doing both. Every computer has an address that other computers can refer to.

When you click on a webpage, the **web browser** of your computer (e.g., chrome, safari, etc.) makes a data request to the **web server** of that page (a database where all the info about that page are stored), and gets back a response.

&lt;img src="request_response.png" width="50%" style="display: block; margin: auto;" /&gt;

Image Source [at this link](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor)

---

### How do computer interact on the web? Example

Navigating the web basically means **sending a bunch of requests to different servers and asking back different files** These files are mainly written in html and other languages.

For example, if you type `https://macss.uchicago.edu/current-student-resources` into your web browser and hit enter, these steps occurs under the hood:

1. your web browser (chrome, safari, etc.) translates what you typed into a HTTP request. That request tells the web server that stores all `macss` info that you would like to access the specific piece of info stored at `/current-student-resources` and that the protocol to use is the `https`

1. the web server that hosts `macss` receives your request and sends back to your web browser an https response and the response content (a bunch of files written in HTML)

1. your browser receives transforms the response content into a nice visual display that might include texts, graphics, hyperlinks, etc.

--

When we perform these operations with the goal of scraping data, we use R packages that perform these steps for us (either via APIs or without)!

---

class: inverse, middle

## Concept 2: What is behind a website

---

### What is behind a website

A website is made of the following elements:

+ **HTML**, which means *HyperText Markup Language*, is the core element of a website. HTML uses tags to organize the webpage (i.e., makes the text bold, creates paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden

+ **CSS**, which means *Cascading Style Sheets*: to make the page looks nice

+ **Javascript (JS)** code: to add interactive elements to the page (you need "dynamic web scraping" techniques to scrape JS code, not covered in this course)

+ **Other stuff** such as images, hyperlinks, videos or multimedia

---

### What is behind a website

Knowing how read the HTML (and CSS) language, is fundamental when we crape the website directly. When we use an API and/or an API wrapper, this is less important, but still useful. 

Our goal: lean how to read HTML and CSS languages

---

class: inverse, middle

## HTML and CSS 

---

### HTML: HyperText Markup Language

* most important thing we need to learn for web scraping
* makes the "skeleton" or structure of a website
* it is made of tags 
* looks messy, but follows a hierarchical tree-like structure of tags within tags (everything marked with `&lt;&gt;` is a tag)

Standard HTML syntax:
```
   &lt;html&gt;
     &lt;head&gt;
        &lt;title&gt;general info about the page&lt;/title&gt;
     &lt;/head&gt;
     &lt;body&gt;
       &lt;p&gt;a paragraph with some text about the page&lt;/p&gt;
       &lt;p&gt;another paragraph with more text&lt;/p&gt;
       &lt;p&gt;...&lt;/p&gt;
     &lt;/body&gt;
   &lt;/html&gt;
   
```

Tree-like structure:[click here](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108) for a visual example


---

### HTML Tags

* tags are organized in a tree-like structure and are nested within each other
* tags go in pairs: one on each end of the content that they display; for example `&lt;p&gt;ciao&lt;/p&gt;` only the word "ciao" shows up on the webpage (the `/` signals the end of the tag)
* tags can have attributes which provide more information

--

Let's examine the html tags stucture more closely!

&lt;!-- add more info here
https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping
* more frequently used tags
* more on tags attributes
* CSS 
* CSS and HTML
* see staff from my Python course, lecture 2 (e.g. every page is different, no perfect structure,etc.)
--&gt;

---

### HTML Tags

* `html`
    * `head`
        * `title`
        * `a href`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

### HTML Tags

* `html`
    * `head`
        * `title`
        * `a href` -- links
        * `script` -- code
    * `body`
        * `div` -- generic container for content (block)
            * `p` -- paragraph
                * `b` -- bold formatting
            * `span` -- generic container for content (in line)
        * `table`
            * `tr` -- row of cells
                * `td` -- actual cell element 
                * `td`
        * `img`


---

### HTML Tags: example

.small[
```html
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Title&lt;/title&gt;
    &lt;a href="http://github.com"&gt;GitHub&lt;/a&gt;
    &lt;script src="https://c.js"&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;
      &lt;p&gt;Click &lt;b&gt;here&lt;/b&gt; now.&lt;/p&gt;
      &lt;span&gt;Frozen&lt;/span&gt;
    &lt;/div&gt;
    &lt;table style="width:100%"&gt;
      &lt;tr&gt;
        &lt;td&gt;Kristen&lt;/td&gt;
        &lt;td&gt;Bell&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/table&gt;
  &lt;img src="http://ia.media-imdb.com/images.png"/&gt;
  &lt;/body&gt;
&lt;/html&gt;
```
]
Find: (1) the text content `Frozen`; (2) the GitHub link and the text content `GitHub`

---

### HTML Tags: example

**Find the text content `Frozen`:**

```html
&lt;span&gt;Frozen&lt;/span&gt;
```

* `&lt;span&gt;&lt;/span&gt;` - tag name
* `Frozen` - content as text

&lt;!--In this ex. there is only one span tag: what if the HTML has multiple span tags?--&gt;
--

**Find the GitHub link and text content `GitHub`:**

```html
&lt;a href="http://github.com"&gt;GitHub&lt;/a&gt;
```

* `&lt;a&gt;&lt;/a&gt;` - tag name
* `href` - tag attribute (argument)
* `"http://github.com"` - tag attribute (value)
* `GitHub` - content as text

---

### HTML references for your scraping projects:

* HTML overview: https://www.w3schools.com/html/html_intro.asp
* List of tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element 

---

### CSS: Cascading Style Sheet

Often a website has HTML + CSS:
* **HTML defines the content** 
* **CSS defines the format** 

This is an example of CSS code. Notice the `span` HTML tag can be styled using CSS `color`: 

```css
span {
  color: #ffffff;
}

table.data {
  width: auto;
```
--

Most websites use HTML with tags such `class` and `id` to provide “hooks” for their CSS. This way the CSS "knows" where to apply CSS stylistic elements.

Example: https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/class

&lt;!-- https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping 
[CSS diner](http://flukeout.github.io) --&gt;

---

## HTML + CSS code: example 

.pull-left[

```html
&lt;body&gt;
    &lt;table id="content"&gt;
        &lt;tr class='name'&gt;
            &lt;td class='firstname'&gt;
                Kurtis
            &lt;/td&gt;
            &lt;td class='lastname'&gt;
                McCoy
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr class='name'&gt;
            &lt;td class='firstname'&gt;
                Leah
            &lt;/td&gt;
            &lt;td class='lastname'&gt;
                Guerrero
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;
&lt;/body&gt;
```

]

.pull-right[

Find the HTML/CSS code to extract:
1. The entire table
1. The general element(s) containing first names
1. Just the specific element "Kurtis"

]

---

class: inverse, middle

## Practice all of this in R with the `rvest` package

  ### `rvest`: https://rvest.tidyverse.org/ 
  ### Scraping presidential statements

---

### Using `rvest` to read HTML

**The R package [`rvest`](`https://rvest.tidyverse.org/`) allows us to:**
1. Collect and read the HTML source code of a webpage
1. Find and keep the specific HTLM/CSS elements that we want from that webpage:
  * by HTML tags or attributes
  * by CSS selectors

--

**Let’s start with step one:**

* We use the `read_html` function from the `rvest` package to call the URL we want data from, grab the HTML response, and store it as an object.
* We are going to scrape data from this URL: `https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration`

---

### Get the page with `read_html`


```r
library(tidyverse)
library(rvest)

url &lt;- "https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration"

dwight &lt;- read_html(x = url)

dwight
```
--

This is not very informative. We can do better! How? `rvest` lets us find and grab the specific HTLM/CSS elements that we want from a webpage:
  * by HTML tags or attributes
  * by CSS selectors

---

### Find specific elements with `html_elements`

For example, if we want to find **all `a` elements** in the HTML of our webpage (which we saved in the object `dwight`), we run the following code:


```r
html_elements(x = dwight, css = "a")
```

--

Observe the output: 
* Many elements on the same page have the same HTML tag. So, if we search for all `a` tags, we likely get a lot of stuff, much of which we do not want.
* We can be more precise! For example, we might want to find **only the element that contains the document's speaker: "Dwight D. Eisenhower"**. How?
  * find it on the webpage
  * modify the above code accordingly

---

### Find specific elements with `html_elements`

To find a specific element, **we need to inspect the HTML of the website.** We can do so in two ways:

1. **Directly**: by right clicking on the page and select "Inspect" (notice here we need the content of the specific `a` tag, which is nested under `&lt;h3 class="diet-title"&gt;`).

--

2. Using the **SelectorGadget**: 
* See [here](https://selectorgadget.com/) and [here](https://rvest.tidyverse.org/articles/articles/selectorgadget.html)
* Drag the SelectorGadget link into your browser's bookmark bar
* Navigate to a webpage and open the SelectorGadget bookmark
* Click on the item to scrape, it will turn green
* Click on yellow items you do not want to scrape (scroll up and down to check)
* Copy the selector to use with `html_elements()`

I generally rely on method 1 or start with the SelectorGadget and confirm with method 1.

---

### Find specific elements with `html_elements`

Finally, we are ready to find **only the element that contains the document's speaker: "Dwight D. Eisenhower".** We modify the previous code accordingly:


```r
html_elements(x = dwight, css = ".diet-title a")
```

--

Once we have identified the element(s) of interest, usually we want to **access further information included in those elements**. Oftentimes this means two things: text and attributes, and `rvest` has two handy functions: 
* `html_text2()` for text
* `html_attr()` for attributes

---

### Get the text of elements with `html_text2()`


```r
speaker_name &lt;- html_elements(dwight, ".diet-title a") %&gt;% 
  html_text2() 

speaker_name
```
--

### Get the attributes of elements with `html_attr()`


```r
speaker_link &lt;- html_elements(dwight, ".diet-title a") %&gt;% 
  html_attr("href") # note a is the tag, href is its attribute

speaker_link
```

We can keep using `html_text2()` and `html_attr()` to select other things of interest such as: this statement's date, its title, or its text body.

---

### Date of statement

As a string (character):

```r
date &lt;- html_elements(x = dwight, css = ".date-display-single") %&gt;%
  html_text2()

date
```

As a date (double of class "Date"):

```r
library(lubridate)

date &lt;- html_elements(x = dwight, css = ".date-display-single") %&gt;%
  html_text2() %&gt;%
  mdy() # format the element text using lubridate

date
class(date)
```

---

### Title


```r
title &lt;- html_elements(x = dwight, css = "h1") %&gt;%
  html_text2()
title
```

--

### Text


```r
text &lt;- html_elements(x = dwight, css = "div.field-docs-content") %&gt;%
  html_text2()

# display the first 1,000 characters
text %&gt;% str_sub(1, 1000) 
```
 
**Now we know how to extract, the speaker, date, title, and full text from this document!**

---

### Make a function

Think: **Why are we doing through all this effort to scrape just one page?**

Make a function called `scrape_docs` that:

- Take as argument an URL of an single webpage
- Get the HTML of that page 
- Scrapes it
- Returns a data frame containing the document's
    - Date
    - Speaker
    - Title
    - Full text

---

.small[

```r
scrape_doc &lt;- function(url) {
  
  # get HTML page
  url_contents &lt;- read_html(x = url)
  
  # extract elements we want
  date &lt;- html_elements(x = url_contents, css = ".date-display-single") %&gt;%
    html_text2() %&gt;% mdy()
  
  speaker &lt;- html_elements(x = url_contents, css = ".diet-title a") %&gt;%
    html_text2()
  
  title &lt;- html_elements(x = url_contents, css = "h1") %&gt;%
    html_text2()
  
  text &lt;- html_elements(x = url_contents, css = "div.field-docs-content") %&gt;%
    html_text2()
  
  # store in a data frame and return it
  url_data &lt;- tibble(
    date = date,
    speaker = speaker,
    title = title,
    text = text
  )
  return(url_data)
}
```
]

---

### Call the function to scrape documents from the UCSB website:


```r
url_1 &lt;- "https://www.presidency.ucsb.edu/documents/letter-t-keith-glennan-administrator-national-aeronautics-and-space-administration"
scrape_doc(url_1)
```


```r
url_2 &lt;- "https://www.presidency.ucsb.edu/documents/letter-the-president-the-senate-and-the-speaker-the-house-representatives-proposing"
scrape_doc(url_2)
```

**Challenge**: How could we further automate our scraper so we do not have to pass 4000+ URLs (that's the amount of URLs in `https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters`) each time?
* collect all URLs on that page, and store them in a list or character vector
* notice each page has about 10 URLs, so we need to tell the scraper to turn page
* apply our `scrape_doc` function to the list of URLs, one at a time 

---

class: inverse, middle

## Practice scraping

### Download today's class materials

---

DROP? 

### Example 01: Sperling's Best Places 

&gt; **Task 1:** look up the cost of living for your hometown on Sperling's Best Places website: http://www.bestplaces.net/ and extract it with `html_elements()` and `html_text2()`

&gt; **Task 2:** get the first table with `html_table()` and put it to a data frame

&gt; **Task 3:** extract the climate statistics table of your hometown 

### Example 02: Movie information from IMDb 


---

class: inverse, middle

## Scraping challenges, tips, and ethics 


---

### Scraping: General tips

* Confirm that there is no R package and no API
* Make sure your code scrapes only what you want (and not additional information)
* Save and store the content of what you scrape, so to avoid scraping it again

---

### Scraping: Challenges &amp; Solutions

* **Variety:** every website is different, so every website requires a new project

* **Bad formatting**: websites should be built using "logical formatting" following a properly nested HTML structure. In practice, that's often not the case

* **Change:** the same website might change over time, so you might find that your code of a few months ago does not work anymore. The good news is that, usually, it takes only a few changes to run it again

* **Limits:** some websites set a max amount of data you can scrape at once, for example 50 pages or 2500 articles max. The solution is to break your requests into "chunks"

* **Messy:** the scraped data are usually a bit messy, and they need to be cleaned (regex is helpful here)

* **Dynamic Scraping:** this not really a challenge but something to keep in mind: many websites incorporate Javascript dynamic parts, which require advanced scraping skills

---

### Scraping: Ethics

* **Private data (not OK) VS. Public data (OK):** If there is a password, it is private data. For example, it is not OK to scrape data from an online community where only logged-in users can access posts (unless you use the API provided by the website and follow its rules). **In general: if the website has an API, use it.**

* **Check the `robots.txt`** before you scrape by adding `/robots.txt` at the end of your URL. For example, for the NYT Robot File, type: `https://www.nytimes.com/robots.txt`. The star after "User-agent" means "the following is valid for all robots"; things you cannot scrape are under "Disallow". More info [here](https://www.robotstxt.org/robotstxt.html)

* **Read the website’s Terms of Service (ToS):** legal rules you agree to observe in order to use a service. Violating ToS exposes you to the risk of violating CFAA or "Computer Fraud &amp; Abuse Act", which is a federal crime.

* **If you are scraping lots of data:** use `rvest` together with `polite`. The polite package ensures that you’re respecting the `robots.txt` and not hammering the site with too many requests.

* For more info on ethical issues, check the **hiQ Labs v. LinkedIn** lawsuit case.

---

### Acknowledgments 

The content of these slides is derived in part from Benjamin Soltoff’s “Computing for the Social Sciences” course materials, licensed under the CC BY NC 4.0 Creative Commons License. Any errors or oversights are mine alone.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLines": true,
"highlightLanguage": "r",
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
