---
title: "Text analysis:  <br /> Topic Modeling and Sentiment Analysis"
author: "MACSS 30500 <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "24px",
  code_font_size = "20px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
xaringanExtra::use_panelset()
```

```{r pkgs, include = FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE
)

library(tidyverse)
library(tidymodels)
library(tidytext)
library(themis)
library(rjson)
library(topicmodels)
library(here)
library(patchwork)
library(tictoc)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

class: inverse, middle

## Agenda

* Review
* Topic Modeling (LDA)
* Sentiment Analysis

---

class: inverse, middle

## Review

---

### General workflow in four steps

Wide range of methods, but similar workflow. We can think at it as a 4-step process:

1. Obtain your textual data

1. Data pre-processing 

1. Data transformation (from words to numbers)

1. Data analysis (wide range of methods; content of previous steps depends on the data analysis)


---

### 4. Data analysis

* Basic exploratory techniques --  Chapter 1, 3, and 4 of the book Tex Mining with R
    * Word frequencies: using simple counts or using a method called TF-IDF
    * Relationship between words: using n-grams or using correlations
    
* Approaches based on simple NLP techniques -- Chapter 2 and 6 
  * Sentiment analysis 
  * Topic modeling
  
* Approaches based on complex NLP techniques (advanced sentiment analysis with BERT or Vader methods; Machine translation; complex question answering; various neural networks for NLP)  

<!--
Last lecture's class materials (R code): updated 
-->

---

class: inverse, middle

## Topic Modeling

---

## Topic Modeling: definition

Method to organize large collections of textual information (without actually reading them).

How? 

**By finding groups of words ("topics") that go together:** words that co-occur more frequently that chance alone would predict are assigned to the same topic. 

If some words appear frequently together, it means that they might show an underlying common topic that is semantically interpretable by humans (what we would call a theme or a discourse).

---

## Topic Modeling: examples


* Organizing 1.2M books into 2000 topics: David Mimno [slides here](https://mimno.infosci.cornell.edu/slides/details.pdf) and [video here](https://vimeo.com/53080123). 

* Book Ch. 6: a vandal has broken into your house, and torn apart 4 books into individual chapters; you are left with the task of putting the books back together (but you have no metadata available)

<!--
Great Expectations by Charles Dickens
The War of the Worlds by H.G. Wells
Twenty Thousand Leagues Under the Sea by Jules Verne
Pride and Prejudice by Jane Austen
-->

---

## Topic Modeling VS other approaches

We have seen: 

* **Word Frequency** looks at the exact recurrence of each word in a corpus (using a simple count or a weighted count with tf-idf).
* **Correlations** looks at how often *two words* appear together (using the ratio of how often the two words appear together / how often they appear separately).

Instead:

* **Topic Modeling** looks at how often *groups of words* appear together (using probability: words that co-occur more frequently that chance alone would predict are assigned to the same topic).

---

## Topic Modeling: intuition

<!-- One way to think about how TMâ€¦ -->

Imagine working through a set of newspaper articles with a set of highlighters. 

As you read through the articles you might find different topics. So, you decide to use a different color to highlight the key words for each topic within each article.

--

When you were done, you could do two things:
* take all words of the same color and create separate lists (one list per each topic)
* for each article observe which color is more frequent (that article must be mostly talking about that topic)
  
<!--
Let's imagine you are reading 10 articles
Idea here is that each of them will have one or more prevalent theme, but it will also talk about other themes. So if you are identifying 4 themes in total across 10 articels, you might be able to group them together based on their prevalent theme or TOPIC.
Method developed after the following intuition: the meaning of words is dependent upon the broader context in which they are used. 
-->

---

## Topic Modeling: intuition

```{r fig.align = "center", echo = FALSE, out.width = "80%"}
knitr::include_graphics(path = "tm.png", error = FALSE)
```

---

## Topic Modeling: LDA

Topic Modeling is the name of a family of algorithms that provide **unsupervised classification** of a collection of documents.

--

The most common algorithm is **LDA or Latent Dirichlet Allocation**. LDA assumes that:

1. Every document in a corpus contains a mix of topics that are found throughout the entire corpus

1. Every topic is made of a (limited) mix of characteristic words, which tend to frequently occur together whenever the topic is displayed

---

## Topic Modeling: LDA

The **topics are hidden ("latent"):** we can only observe the documents and words, not the topics themselves. The goal of LDA is using **probability** to infer such topics by looking at the words and documents.

More formally: LDA computes the conditional posterior distribution of latent variables, e.g. topics, given the observed variables, e.g. words in documents. See Blei reading for more!

---

## Topic Modeling: LDA

The process is as follows:

1. The researcher begins by setting an arbitrary number of topics (`k`) for the whole corpus (collection of documents)

1. The LDA algorithm (Gibbs or VEM sampling are the most common) creates simultaneously two distributions:
    * a first randomly chosen **word-topic probability, "beta"** distribution of topics by all words in the collection
    * a first randomly chosen **document-topic probability, "gamma:** distribution of topics by all documents in the collection 

1. Through reiterative attempts, the algorithm adjusts these initial distributions to provide a set of probabilities for every word-topic pair and for every document-topic pair. 

---

## Topic Modeling: LDA

**LDA produces two types of output:**

1. Words most frequently associated with each of the `k` topics specified by the researcher 

1. Documents most frequently associated with each of the `k` topics (the researcher defines a probability theresold)

---

## Topic Modeling: LDA simple example

We start with a simple example, then move to a real example in R. 

Imagine we have a corpus with the following five documents (each document is one sentence):

**Document 1**  I ate a banana and spinach smoothie for breakfast.

**Document 2**  I like to eat broccoli and bananas.

**Document 3**  Puppies and kittens are cute.

**Document 4**  My sister adopted a kitten yesterday.

**Document 5**  This cute hamster is eating a piece of broccoli.

---

## Topic Modeling: LDA simple example

**First, transform the textual data in the shape required for LDA:**
* input:
  * raw data
* output:
  * create a vocabulary (tokenize, remove stopwords and punctuation, lowercase, stemmatize/lemmatize, etc.)
  * create a document-term matrix

**Then, run LDA:**
* input:
  * document-term matrix (rows documents, columns terms)
  * number of topics to generate (you decide them)
* output:
  * word-topic probabilities
  * document-topic probabilities

---


## Topic Modeling: LDA simple example

The **document-term matrix (DTM)**
* is a way to turn documents into vectors of numbers
* example of building a DTM using the five documents above
* different from tidy text format generated by `unnest_tokens` (specific to R tidy text)  

<!--
NB: the tidy text format is specific to R tidyverse VS. document-term matrix, which is a general format for NLP and has one-document-per-row and one-term-per-column
-->

---


## Topic Modeling: LDA simple example

If we give to LDA the document-term matrix from these five documents, and we ask for 2 topics, LDA might produce something like: 

**Topic A:** 30% eat, 20% broccoli, 15% bananas, 10% breakfast, ... (can label it "food")

**Topic B:** 20% dog, 20% kitten, 20% cute, 15% hamster, .. (can label it "cute animals")

--

**Document 1 and 2** 100% Topic A

**Document 3 and 4**: 100% Topic B 

**Sentence 5:** 60% Topic A, 40% Topic B

---

## Topic Modeling: LDA example in R

Figure 6.1: flowchart of text analysis with topic modeling 

```{r fig.align = "center", echo = FALSE, out.width = "70%"}
knitr::include_graphics(path = "tmwr_0601.png", error = FALSE)
```

---

##  Topic Modeling: LDA example in R

**Converting to and from non-tidy formats, see [Chapter 5](https://www.tidytextmining.com/dtm):**

* Ch. 1-4 focus on the **`tidytext` package**. Input: tidy text (one-token-per-document-per-row using `unnest_tokens()`)

* BUT there are other R packages for NLP that arenâ€™t compatible with tidy text format. For example, to run topic models the most popular package is the  **`topicmodels` package**. Input: matrix (document-term matrix)

The tidytext package provides two verbs that convert between these two formats:
* `tidy()`: from matrix to tidy text
* `cast()`: form tidy text to matrix


---

## Topic Modeling: LDA example in R

Examples from the book:
* Chapter 6 (topic models only) -- we use this
* Chapter 7-9 (three case-study chapters)


---

## Topic Modeling: pros and cons

* Topic models **account for "multiplexity":** a given document is unlikely to fall precisely and fully into a single topic 

* Topic models **do not care about the order of words** â€œdog eats food" is the same as "food eats dog" 

* To determine the "right" number of topics: no fixed rule, it is a try-error process, ultimately **the researcher decides** (there are some metrics, such as perplexity and coherence score but are beyond our goals)

* Topics are **unlabeled:** they are just a bunch of words, it is up to the researcher to read, interpret, and label them

* Topic models do not work well with short documents (aka when vocabulary is small) as they need enough vocabulary variation (e.g. not suitable to classify tweets)

---

## Topic Modeling: important reminder!

Topic Modeling is **no substitute for human interpretation of a text** 

It is way of making educated guesses about how words cohere into different latent themes by identifying patterns in the way they co-occur within documents.

Many people are somewhat disappointed when they discover their model produces uninformative results:
* GIGO: Garbage-In, Garbage-Out (data prepossessing matters a lot with TM)
* needs researcher's knowledge of the subject
* patience and time

---

## LDAvis

LDAvis is a an interactive visualization of LDA model results: https://github.com/cpsievert/LDAvis

It helps to answer the following questions:

1. What is the meaning of each topic?
1. How prevalent is each topic?
1. How do the topics relate to each other?

---

class: inverse, middle

# Sentiment Analysis

---

## Sentiment Analysis

Definition: use of NLP to **study emotional states and subjective information in a text**. 

In practice sentiment analysis is usually applied to:
* determine the polarity of a text: whether a text is positive, negative, or neutral
* detect specific feelings and emotions: angry, sad, happy, etc.

Examples: analyzing costumer feedback, classify movies review

---

## Sentiment Analysis: our approach

Sentiment Analysis is a broad term. There are several ways to perform it. **Our approach** is consistent with Ch. 2 of the book and considers: 

* the text a combination of its individual words, and 
* the sentiment of the text is given by the sum of the sentiment content of its individual words (simple count and sum of how many negative, positive, neutral words a text has)

*"This isnâ€™t the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem."*

---

## The `sentiments` lexicons

Different from Topic Modeling, Sentiment Analysis it is usually a **supervised classification** method: it needs a labeled input (called dictionary or lexicon) that someone (experts!) has created, and we use then use to classify our data. 

--

The tidytext package provides access to three well-established sentiment lexicons developed by researchers (check how they have been :

+ **AFINN** from Finn Ã…rup Nielsen: words classified with on a scale from -5 to +5 (http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html)

+ **bing** from Bing Liu and collaborators: words classified into binary categories, either negative or positive (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)

+ **nrc** from Saif Mohammad and Peter Turney: words classified in multiple categories (http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

See: https://www.tidytextmining.com/sentiment.html#the-sentiments-datasets

Which one is better? elements to evaluate: complexity and what the researchers used to validate (e.g. twitter or literature)

---

## The `sentiments` lexicons

These dictionaries are our **"gold standard"**: they contain labeled sentiments, aka the "true" value. 

--

We use these dictionaries to determine the sentiment of our data by searching for all matches, and simply counting them. For example:

* the word "angry" is classified as negative in the bing dictionary
* we take our collection of documents and count how many times the words "angry" appear in each document
* repeat for all words in the dictionary
* establish a threshold and classify each document accordingly

---

## Sentiment Analysis: examples in R

Examples from the book:
* Chapter 2 (sentiment analysis only)
* Chapter 7-9 (three case-study chapters)

Another example (in today's class-materials): sentiment analysis of Harry Potter textual data. 

