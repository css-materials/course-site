---
title: "Text analysis: fundamentals"
author: "MACSS 30500 <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "24px",
  code_font_size = "20px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
xaringanExtra::use_panelset()
```

```{r pkgs, include = FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(tidytext)
library(scales)
library(here)
library(patchwork)
library(magrittr)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

class: inverse, middle

# Agenda


---

class: inverse, middle

# Basic workflow for text analysis

---

## Basic workflow for text analysis

We can think at the basic workflow as a 4-step process:

1. Obtain your textual data

1. Data cleaning and pre-processing 

1. Data transformation

1. Perform analysis

Let's review each step...

---

## 1. Obtain your textual data

**Common data sources for text analysis:**

* Online (Scraping and/or APIs)
* Databases
* PDF documents
* Digital scans of printed materials

---

## 1. Obtain your textual data

**Corpus and document:**

* Textual data are usually referred to as **corpus**: general term to refer to a collection of texts, stored as raw strings (e.g., a set of articles from the NYT, novels by an author, one or multiple books, etc.)

* Each corpus might have separate articles, chapters, pages, or even paragraphs. Each individual unit is called a **document**. You decide what constitutes a document in your corpus. 

---

## 2. Data cleaning and pre-processing

**Standard cleaning and pre-processing tasks:**

* Tokenize the text (to n-grams)
* Convert to lower case
* Remove punctuation and numbers
* Remove stopwords (standard and custom/domain-specific)
* Remove or replace other unwanted tokens
* Stemming or Lemmatization 

---

## 2. Data cleaning and pre-processing

**Tokenize the text (to n-grams) mean splitting your text into single tokens**. 

**Token:** word, alphanumeric character, !, ?, number, emoticon etc.

Most tokenizers split on white spaces, but they need also consider exceptions such as contractions (I'll, dog's), hyphens in or between words (e-mail, co-operate, take-it-or-leave, 30-year-old).

**N-gram:** a contiguous sequence of n items from a given text (items can be syllables, letters, words, etc.). We usually keep unigrams (the single word), but there instances in which bigrams are helpful: for example "Joe Biden" is a bi-gram.  
---

## 2. Data cleaning and pre-processing

**Remove stopwords** (standard and custom/domain-specific) 
* Examples: the, is, are, a, an, in, etc.
* Why we want to remove them? 
* Example: if you are working on a corpus that talks about "President Biden" you might want to add "Biden" among your stop words

---

## 2. Data cleaning and pre-processing

**Stemming** and **lemmatization** are similar in that both aim at simplifying words (aka tokens) to their base form but they do it differently. Why do we want to do it?

--

**Stemming:** reducing a token to its **root stem** by brutally removing parts from them 
* Examples: dogs become dog, walked becomes walk
* Faster, but not always accurate. Example: caring becomes car, changing becomes chang, better becomes bett

**Lemmatization:** reducing a token to its **root lemma** by using their meaning, so the token is converted to the concept that it represents
* Examples: dogs become dog, walked becomes walk, 
* Slower, but more accurate. Example: caring becomes care, changing becomes change, better becomes good

---

## 2. Data cleaning and pre-processing

More advanced pre-processing tasks (only applied to specific analyses):

* POS or Part-Of-Speech tagging (nouns, verbs, adjectives, etc.) 
* NER or Named Entity Recognition tagging (person, place, company, etc.)
* Parsing

---

## 3. Data transformation

Transformation means *converting the text into numbers*, e.g. some sort of quantifiable measure that a computer can process.

Usually you want to transform your raw textual data (your document) into a vector of countable units:

* Bag-of-words model: creates a document-term matrix (one row for each document, and one column for each term)
* Word embedding models

---

## 4. Perform analysis

This week we learn the following:

* Basic exploratory analysis
    * Word frequency
    * TF-IDF (weighted version of word frequency)
    * Correlations
    
* More Advanced
    * Sentiment analysis
    * Topic modeling

---

class: inverse, middle

# Text analysis with R tidyverse 


---

### The tidy text format

There are different ways to complete all steps in R, and different packages have their own approach. We learn how to perform these operations within the tidyverse. 

For the data cleaning and pre-processing step: we start by converting text into a tidy format, which follows the same principle of tidy analysis we have learned so far.

Take a look at [Figure 1.1](https://www.tidytextmining.com/tidytext.html#fig:tidyflow-ch1) from the assigned readings

---

### The tidy text format

A tidy text format is defined as **a table with one-token-per-row** (this is different from the document-term matrix, which has one-document-per-row and one-term-per-column).

Steps:

* take your text
* put into a tibble
* convert into the tidy text format using `unnest_tokens()` 
  * punctuation is automatically removed
  * lower case is automatically applied

See [`tidytext`](https://github.com/juliasilge/tidytext) for more info

---

### The tidy text format

Examples of basic exploratory text analysis using R tidyverse.
These examples all in your in-class materials for today and might be useful for your HW8.

**Example 1:** from book "1.2 Emily Dickinson example"

**Example 2:** from the book "1.3 Jane Austen example"

**Example 3:** How often is each U.S. state mentioned in a popular song? We’ll define popular songs as those in Billboard’s Year-End Hot 100 from 1958 to the present. Data from  Billboard Year-End Hot 100 (1958-present) and the Census Bureau ACS

**More examples:** in the assigned readings and suggested resources 
